#!/usr/bin/env python3
"""
Data Structure Optimization Test Suite
======================================
Comprehensive testing of DataFrame column reduction and access pattern optimizations.
"""

import pandas as pd
import numpy as np
import time
import sys
import os
from bigbar import (
    lazy_atr_computation, 
    cleanup_atr_columns,
    precompute_atr_values,
    precompute_week_boundaries,
    load_data,
    run_backtest_optimized
)

def create_test_data(n_rows=5000):
    """Create test data for performance testing."""
    print(f"Creating test data with {n_rows} rows...")
    
    # Create time series
    dates = pd.date_range('2023-01-01', periods=n_rows, freq='1min')
    
    # Generate price data with realistic patterns
    np.random.seed(42)
    base_price = 100.0
    price_changes = np.random.normal(0, 0.1, n_rows)
    prices = base_price + np.cumsum(price_changes)
    
    # Add some volatility
    volatility = np.random.uniform(0.5, 2.0, n_rows)
    
    # Create OHLC data
    data = {
        'time': dates,
        'open': prices + np.random.normal(0, 0.1, n_rows),
        'high': prices + np.random.uniform(0, 1.0, n_rows) * volatility,
        'low': prices - np.random.uniform(0, 1.0, n_rows) * volatility,
        'close': prices + np.random.normal(0, 0.1, n_rows),
        'volume': np.random.randint(100, 1000, n_rows)
    }
    
    df = pd.DataFrame(data)
    df.to_csv('test_data_structure.csv', index=False)
    print(f"Test data saved to test_data_structure.csv")
    return 'test_data_structure.csv'

def test_column_reduction():
    """Test the column reduction functionality."""
    print("\n" + "="*60)
    print("COLUMN REDUCTION TEST")
    print("="*60)
    
    # Load test data
    filepath = create_test_data(2000)
    df = load_data(filepath)
    
    print(f"Original DataFrame shape: {df.shape}")
    print(f"Original columns: {list(df.columns)}")
    
    # Pre-compute ATR values for full range
    print("\nPre-computing ATR values for periods 10-100...")
    df_full = precompute_atr_values(df.copy(), 10, 100)
    
    atr_columns_full = [col for col in df_full.columns if col.startswith('ATR_')]
    print(f"Full ATR columns: {len(atr_columns_full)}")
    print(f"Full DataFrame shape: {df_full.shape}")
    
    # Test lazy ATR computation
    print("\nTesting lazy ATR computation...")
    df_lazy = df.copy()
    atr_20 = lazy_atr_computation(df_lazy, 20)
    
    atr_columns_lazy = [col for col in df_lazy.columns if col.startswith('ATR_')]
    print(f"Lazy ATR columns: {len(atr_columns_lazy)}")
    print(f"Lazy DataFrame shape: {df_lazy.shape}")
    
    # Test column cleanup
    print("\nTesting column cleanup...")
    df_cleaned = cleanup_atr_columns(df_full.copy())
    
    atr_columns_cleaned = [col for col in df_cleaned.columns if col.startswith('ATR_')]
    print(f"Cleaned ATR columns: {len(atr_columns_cleaned)}")
    print(f"Cleaned DataFrame shape: {df_cleaned.shape}")
    
    # Memory usage comparison
    print("\nMemory usage comparison:")
    memory_full = df_full.memory_usage(deep=True).sum() / 1024**2  # MB
    memory_lazy = df_lazy.memory_usage(deep=True).sum() / 1024**2  # MB
    memory_cleaned = df_cleaned.memory_usage(deep=True).sum() / 1024**2  # MB
    
    print(f"  Full ATR columns: {memory_full:.2f} MB")
    print(f"  Lazy ATR (single): {memory_lazy:.2f} MB")
    print(f"  Cleaned ATR: {memory_cleaned:.2f} MB")
    
    print(f"\nMemory savings:")
    print(f"  Lazy vs Full: {(memory_full - memory_lazy):.2f} MB ({((memory_full - memory_lazy) / memory_full * 100):.1f}%)")
    print(f"  Cleaned vs Full: {(memory_full - memory_cleaned):.2f} MB ({((memory_full - memory_cleaned) / memory_full * 100):.1f}%)")
    
    # Cleanup
    os.remove(filepath)
    return df_full, df_lazy, df_cleaned

def test_access_pattern_optimization():
    """Test the access pattern optimizations."""
    print("\n" + "="*60)
    print("ACCESS PATTERN OPTIMIZATION TEST")
    print("="*60)
    
    # Create test data
    filepath = create_test_data(1000)
    df = load_data(filepath)
    
    # Pre-compute ATR and week boundaries
    df = precompute_atr_values(df, 20, 20)
    df = precompute_week_boundaries(df)
    df = df.dropna(subset=['ATR_20'])
    
    print(f"Test DataFrame shape: {df.shape}")
    
    # Test traditional index-based access
    print("\nTesting traditional index-based access...")
    start_time = time.time()
    
    traditional_results = []
    for i in range(100, len(df) - 5):
        # Traditional approach
        bar1 = (df['Close'].iloc[i-4] - df['Open'].iloc[i-4])
        bar2 = (df['Close'].iloc[i-3] - df['Open'].iloc[i-3])
        bar3 = (df['Close'].iloc[i-2] - df['Open'].iloc[i-2])
        
        weighted_sum = (1 * bar1) + (2 * bar2) + (3 * bar3)
        traditional_results.append(weighted_sum)
    
    traditional_time = time.time() - start_time
    print(f"Traditional access time: {traditional_time:.4f}s")
    
    # Test vectorized access
    print("\nTesting vectorized access...")
    start_time = time.time()
    
    vectorized_results = []
    for i in range(100, len(df) - 5):
        # Vectorized approach
        prev_indices = [i-4, i-3, i-2]
        prev_closes = df['Close'].iloc[prev_indices]
        prev_opens = df['Open'].iloc[prev_indices]
        
        bar_diffs = prev_closes - prev_opens
        weights = np.array([1, 2, 3])
        weighted_sum = np.dot(bar_diffs.values, weights)
        vectorized_results.append(weighted_sum)
    
    vectorized_time = time.time() - start_time
    print(f"Vectorized access time: {vectorized_time:.4f}s")
    
    # Verify results are identical
    results_match = np.allclose(traditional_results, vectorized_results)
    print(f"\nResults match: {results_match}")
    
    if results_match:
        speedup = traditional_time / vectorized_time
        print(f"Speedup: {speedup:.2f}x")
    
    # Cleanup
    os.remove(filepath)
    return traditional_time, vectorized_time

def test_full_optimization_pipeline():
    """Test the complete optimization pipeline."""
    print("\n" + "="*60)
    print("FULL OPTIMIZATION PIPELINE TEST")
    print("="*60)
    
    # Create test data
    filepath = create_test_data(3000)
    
    print("Testing full optimization pipeline...")
    start_time = time.time()
    
    # Load data
    df = load_data(filepath)
    
    # Pre-compute ATR for specific period (lazy approach)
    df = precompute_atr_values(df, 20, 20)
    
    # Pre-compute week boundaries
    df = precompute_week_boundaries(df)
    
    # Remove NaN values
    df = df.dropna(subset=['ATR_20'])
    
    pipeline_time = time.time() - start_time
    print(f"Pipeline execution time: {pipeline_time:.4f}s")
    print(f"Final DataFrame shape: {df.shape}")
    
    # Test backtest with optimized data
    print("\nTesting backtest with optimized data...")
    start_time = time.time()
    
    try:
        stats, bt = run_backtest_optimized(filepath, print_result=False, atr_period=20)
        backtest_time = time.time() - start_time
        print(f"Backtest execution time: {backtest_time:.4f}s")
        print(f"Trades executed: {len(stats._trades) if hasattr(stats, '_trades') else 0}")
    except Exception as e:
        print(f"Backtest failed: {e}")
        backtest_time = 0
    
    # Cleanup
    os.remove(filepath)
    if os.path.exists('bigbar_trades.csv'):
        os.remove('bigbar_trades.csv')
    
    return pipeline_time, backtest_time

def main():
    """Run all optimization tests."""
    print("Data Structure Optimization Test Suite")
    print("=" * 60)
    
    try:
        # Test column reduction
        df_full, df_lazy, df_cleaned = test_column_reduction()
        
        # Test access pattern optimization
        traditional_time, vectorized_time = test_access_pattern_optimization()
        
        # Test full pipeline
        pipeline_time, backtest_time = test_full_optimization_pipeline()
        
        # Summary
        print("\n" + "="*60)
        print("OPTIMIZATION SUMMARY")
        print("="*60)
        
        print(f"\nColumn Reduction:")
        memory_full = df_full.memory_usage(deep=True).sum() / 1024**2
        memory_lazy = df_lazy.memory_usage(deep=True).sum() / 1024**2
        memory_cleaned = df_cleaned.memory_usage(deep=True).sum() / 1024**2
        
        print(f"  Memory usage reduction: {(memory_full - memory_lazy):.2f} MB ({((memory_full - memory_lazy) / memory_full * 100):.1f}%)")
        print(f"  Columns reduced from {len(df_full.columns)} to {len(df_lazy.columns)}")
        
        print(f"\nAccess Pattern Optimization:")
        if vectorized_time > 0:
            speedup = traditional_time / vectorized_time
            print(f"  Access speed improvement: {speedup:.2f}x")
            print(f"  Time saved: {traditional_time - vectorized_time:.4f}s")
        
        print(f"\nPipeline Performance:")
        print(f"  Data preparation time: {pipeline_time:.4f}s")
        print(f"  Backtest execution time: {backtest_time:.4f}s")
        
        print(f"\nðŸŽ¯ OPTIMIZATION BENEFITS:")
        print(f"  âœ“ Reduced memory usage by eliminating redundant ATR columns")
        print(f"  âœ“ Improved cache efficiency with vectorized operations")
        print(f"  âœ“ Faster column access with cached series references")
        print(f"  âœ“ Better memory layout for improved performance")
        
    except Exception as e:
        print(f"Test failed with error: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
